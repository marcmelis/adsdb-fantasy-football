{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from datetime import timedelta\n",
    "from dtype_dictionaries import create_dtype_dict\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metdata (weather) imputation\n",
    "All the data is weather related and has a time dimension. \n",
    "\n",
    "#### Numeric values\n",
    "(a) Sparse missings\n",
    "\n",
    "Impute by interpolating the time series from the same Station\n",
    "\n",
    "(b) Dens missings\n",
    "\n",
    "Impute by taking averaging nearby stations\n",
    "\n",
    "\n",
    "#### Wind direction \n",
    "\n",
    "Take the direction and calculate the mean degrees of the three closest weather stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metoffice_01_22_2023-10-13\n",
      "weather_station_locations_2023-10-20\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('../data/formatted_zone/formatted_zone.db')\n",
    "table_name = 'Metoffice_01_22_2023-10-13'\n",
    "dtype_dict, date_columns = create_dtype_dict(table_name)\n",
    "df = pd.read_sql_query(f\"SELECT * FROM \\\"{table_name}\\\";\", conn, parse_dates=date_columns)\n",
    "\n",
    "dtype_dict, date_columns = create_dtype_dict('weather_station_locations_2023-10-20')\n",
    "loc_df = pd.read_sql_query(f\"SELECT * FROM \\\"{'weather_station_locations_2023-10-20'}\\\";\", conn, parse_dates=date_columns)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(lat1, lon1, lat2, lon2):\n",
    "    # Assuming the Britain can be approximated as a flat plane, calculate the distance using Pythagoras' theorem\n",
    "    lat_diff = lat2 - lat1\n",
    "    lon_diff = lon2 - lon1\n",
    "    distance = math.sqrt(lat_diff**2 + lon_diff**2)\n",
    "\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wdir_to_deg(wdir): \n",
    "    wdir_dict = {\n",
    "        \"N\": 0,\n",
    "        \"E\": 90,\n",
    "        \"S\": 180,\n",
    "        \"W\": 270\n",
    "    }\n",
    "\n",
    "    for i, c in enumerate(wdir): \n",
    "        if i == 0: deg = wdir_dict[wdir[len(wdir) - 1 - i]]\n",
    "        else: \n",
    "            deg = (deg + wdir_dict[wdir[len(wdir) - 1 - i]]) / 2\n",
    "\n",
    "    return deg\n",
    "\n",
    "def deg_to_wdir(deg): \n",
    "    wind_directions = [\n",
    "        ('N', (345, 15)),\n",
    "        ('NNE', (15, 30)),\n",
    "        ('NE', (30, 60)),\n",
    "        ('ENE', (60, 75)),\n",
    "        ('E', (75, 105)),\n",
    "        ('ESE', (105, 120)),\n",
    "        ('SE', (120, 150)),\n",
    "        ('SSE', (150, 165)),\n",
    "        ('S', (165,195)),\n",
    "        ('SSW', (195,210)),\n",
    "        ('SW', (210,240)),\n",
    "        ('WSW', (240,255)),\n",
    "        ('W', (255,285)),\n",
    "        ('WNW', (285,300)),\n",
    "        ('NW', (300,330)),\n",
    "        ('NNW', (330,345))\n",
    "    ]\n",
    "    \n",
    "    # Loop through the wind direction abbreviations and degree ranges\n",
    "    for direction, (lower, upper) in wind_directions:\n",
    "        if lower <= deg < upper:\n",
    "            return direction\n",
    "\n",
    "    # If the input degrees are outside the defined ranges, return 'N' (North)\n",
    "    return 'N'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolation_threshold = 0.8\n",
    "station_counts = df.Station_name.value_counts() / 2\n",
    "imputed_df = df.__deepcopy__()\n",
    "\n",
    "for column in df.columns:\n",
    "    # count null values per station for this column\n",
    "    # station_null_counts = df.loc[df.loc[:,column].isna()].Station_name.value_counts()\n",
    "    \n",
    "    # iterate through the dataframe\n",
    "    for i, value in enumerate(df[column]):\n",
    "        if pd.isna(value):\n",
    "            station = df.loc[i, \"Station_name\"]\n",
    "            date = df.iloc[i].Date\n",
    "\n",
    "            # check if this station should be interpolated for this column\n",
    "            # perc_missing = station_null_counts[station] / station_counts[station] # percentage based criteria\n",
    "            # if  perc_missing < interpolation_threshold: # TODO check if we can interpolate\n",
    "            \n",
    "            start_date = date - timedelta(days = 1)\n",
    "            end_date = date + timedelta(days = 1)\n",
    "            \n",
    "            prev_measure = np.nan\n",
    "            post_measure = np.nan\n",
    "            t = df.loc[(df.Station_name == station) & (df.Date == start_date)]\n",
    "            if len(t) > 0:\n",
    "                prev_measure = t[column].iloc[0]\n",
    "            t = df.loc[(df.Station_name == station) & (df.Date == end_date)]\n",
    "            if len(t) > 0:\n",
    "                post_measure = t[column].iloc[0]\n",
    "\n",
    "            if not np.isnan(prev_measure) and isinstance(prev_measure, (int, float)) and not np.isnan(post_measure) and isinstance(post_measure, (int, float)): \n",
    "                interpolated_value = np.mean([prev_measure, post_measure])\n",
    "            else: \n",
    "                # mean of nearby stations\n",
    "                lat = loc_df.loc[loc_df.SITE == station].LAT\n",
    "                lon = loc_df.loc[loc_df.SITE == station].LON\n",
    "                loc_df['Distance'] = loc_df.apply(lambda row: euclidean_distance(lat, lon, row['LAT'], row['LON']), axis=1)\n",
    "                # Sort the DataFrame by distance\n",
    "                loc_df = loc_df.sort_values(by='Distance')\n",
    "\n",
    "                # Select the top three closest stations\n",
    "                closest_stations = list(loc_df.head(4).SITE)\n",
    "                closest_df = df.loc[(df.Date == date) & df.Station_name.apply(lambda x: x in closest_stations)]\n",
    "\n",
    "                if column == 'WDIR': \n",
    "                    interpolated_value = deg_to_wdir(closest_df.loc[:,column].apply(wdir_to_deg).mean())\n",
    "                else: \n",
    "                    interpolated_value = closest_df.loc[:,column].mean()\n",
    "            \n",
    "            # print(i, interpolated_value, station, column, date)\n",
    "            imputed_df.loc[i, column] = interpolated_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date            0\n",
       "Station_no      0\n",
       "Station_name    0\n",
       "PRESS           0\n",
       "WDIR            0\n",
       "WSPD            0\n",
       "CLOUD           0\n",
       "TEMP            0\n",
       "TDEW            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation of football-data\n",
    "\n",
    "It is very hard to deal with missing data in the game statistic columns. This just tries to deal with the missing values in the game odds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "football-data_2223_2023-10-13\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('../data/formatted_zone/formatted_zone.db')\n",
    "table_name = 'football-data_2223_2023-10-13'\n",
    "dtype_dict, date_columns = create_dtype_dict(table_name)\n",
    "df = pd.read_sql_query(f\"SELECT * FROM \\\"{table_name}\\\";\", conn, parse_dates=date_columns)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (df.FTR != df.apply(lambda row: \"D\" if row[\"FTHG\"] == row[\"FTAG\"] else \"H\" if row[\"FTHG\"] > row[\"FTAG\"] else \"A\", axis=1)).sum() == 0\n",
    "assert (df.HTR != df.apply(lambda row: \"D\" if row[\"HTHG\"] == row[\"HTAG\"] else \"H\" if row[\"HTHG\"] > row[\"HTAG\"] else \"A\", axis=1)).sum() == 0\n",
    "assert (df.HS < df.HST).sum() == 0\n",
    "assert (df.AS < df.AST).sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 P>2.5 1.25\n",
      "79 P<2.5 3.9\n",
      "79 PC>2.5 1.26\n",
      "79 PC<2.5 3.87\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns[24:]:\n",
    "    backup_column = None\n",
    "    original_columns = None\n",
    "\n",
    "    if   column in [\"B365H\",\"BWH\",\"IWH\",\"PSH\",\"WHH\",\"VCH\"]: \n",
    "        backup_column = \"AvgH\" \n",
    "        original_columns = [\"B365H\",\"BWH\",\"IWH\",\"PSH\",\"WHH\",\"VCH\"]\n",
    "    elif column in [\"B365D\",\"BWD\",\"IWD\",\"PSD\",\"WHD\",\"VCD\"]: \n",
    "        backup_column = \"AvgD\" \n",
    "        original_columns = [\"B365D\",\"BWD\",\"IWD\",\"PSD\",\"WHD\",\"VCD\"]\n",
    "    elif column in [\"B365A\",\"BWA\",\"IWA\",\"PSA\",\"WHA\",\"VCA\"]: \n",
    "        backup_column = \"AvgA\" \n",
    "        original_columns = [\"B365A\",\"BWA\",\"IWA\",\"PSA\",\"WHA\",\"VCA\"]\n",
    "    elif column in [\"B365>2.5\",\"P>2.5\"]: \n",
    "        backup_column = \"Avg>2.5\" \n",
    "        original_columns = [\"B365>2.5\",\"P>2.5\"]\n",
    "    elif column in [\"B365<2.5\",\"P<2.5\"]: \n",
    "        backup_column = \"Avg<2.5\" \n",
    "        original_columns = [\"B365<2.5\",\"P<2.5\"]\n",
    "    elif column in [\"AHh\",\"B365AHH\",\"PAHH\"]: \n",
    "        backup_column = \"AvgAHH\" \n",
    "        original_columns = [\"AHh\",\"B365AHH\",\"PAHH\"]\n",
    "    elif column in [\"B365AHA\",\"PAHA\"]: \n",
    "        backup_column = \"AvgAHA\" \n",
    "        original_columns = [\"B365AHA\",\"PAHA\"]\n",
    "    elif column in [\"B365CH\",\"BWCH\",\"IWCH\",\"PSCH\",\"WHCH\",\"VCCH\"]: \n",
    "        backup_column = \"AvgCH\" \n",
    "        original_columns = [\"B365CH\",\"BWCH\",\"IWCH\",\"PSCH\",\"WHCH\",\"VCCH\"]\n",
    "    elif column in [\"B365CD\",\"BWCD\",\"IWCD\",\"PSCD\",\"WHCD\",\"VCCD\"]: \n",
    "        backup_column = \"AvgCD\" \n",
    "        original_columns = [\"B365CD\",\"BWCD\",\"IWCD\",\"PSCD\",\"WHCD\",\"VCCD\"]\n",
    "    elif column in [\"B365CA\",\"BWCA\",\"IWCA\",\"PSCA\",\"WHCA\",\"VCCA\"]: \n",
    "        backup_column = \"AvgCA\" \n",
    "        original_columns = [\"B365CA\",\"BWCA\",\"IWCA\",\"PSCA\",\"WHCA\",\"VCCA\"]\n",
    "    elif column in [\"B365C>2.5\",\"PC>2.5\"]: \n",
    "        backup_column = \"AvgC>2.5\" \n",
    "        original_columns = [\"B365C>2.5\",\"PC>2.5\"]\n",
    "    elif column in [\"B365C<2.5\",\"PC<2.5\"]: \n",
    "        backup_column = \"AvgC<2.5\" \n",
    "        original_columns = [\"B365C<2.5\",\"PC<2.5\"]\n",
    "    elif column in [\"AHCh\",\"B365CAHH\",\"PCAHH\"]: \n",
    "        backup_column = \"AvgCAHH\" \n",
    "        original_columns = [\"AHCh\",\"B365CAHH\",\"PCAHH\"]\n",
    "    elif column in [\"B365CAHA\",\"PCAHA\"]: \n",
    "        backup_column = \"AvgCAHA\" \n",
    "        original_columns = [\"B365CAHA\",\"PCAHA\"]\n",
    "\n",
    "    \n",
    "    if   column in [\"AvgH\", \"MaxH\"]:        original_columns = [\"B365H\",\"BWH\",\"IWH\",\"PSH\",\"WHH\",\"VCH\"]\n",
    "    elif column in [\"AvgD\", \"MaxD\"]:        original_columns = [\"B365D\",\"BWD\",\"IWD\",\"PSD\",\"WHD\",\"VCD\"]\n",
    "    elif column in [\"AvgA\", \"MaxA\"]:        original_columns = [\"B365A\",\"BWA\",\"IWA\",\"PSA\",\"WHA\",\"VCA\"]\n",
    "    elif column in [\"Avg>2.5\", \"Max>2.5\"]:  original_columns = [\"B365>2.5\",\"P>2.5\"]\n",
    "    elif column in [\"Avg<2.5\", \"Max<2.5\"]:  original_columns = [\"B365<2.5\",\"P<2.5\"]\n",
    "    elif column in [\"AvgAHH\", \"MaxAHH\"]:    original_columns = [\"AHh\",\"B365AHH\",\"PAHH\"]\n",
    "    elif column in [\"AvgAHA\", \"MaxAHA\"]:    original_columns = [\"B365AHA\",\"PAHA\"]\n",
    "    elif column in [\"AvgCH\", \"MaxCH\"]:      original_columns = [\"B365CH\",\"BWCH\",\"IWCH\",\"PSCH\",\"WHCH\",\"VCCH\"]\n",
    "    elif column in [\"AvgCD\", \"MaxCD\"]:      original_columns = [\"B365CD\",\"BWCD\",\"IWCD\",\"PSCD\",\"WHCD\",\"VCCD\"]\n",
    "    elif column in [\"AvgCA\", \"MaxCA\"]:      original_columns = [\"B365CA\",\"BWCA\",\"IWCA\",\"PSCA\",\"WHCA\",\"VCCA\"]\n",
    "    elif column in [\"AvgC>2.5\", \"MaxC>2.5\"]:original_columns = [\"B365C>2.5\",\"PC>2.5\"]\n",
    "    elif column in [\"AvgC<2.5\", \"MaxC<2.5\"]:original_columns = [\"B365C<2.5\",\"PC<2.5\"]\n",
    "    elif column in [\"AvgCAHH\", \"MaxCAHH\"]:  original_columns = [\"AHCh\",\"B365CAHH\",\"PCAHH\"]\n",
    "    elif column in [\"AvgCAHA\", \"MaxCAHA\"]:  original_columns = [\"B365CAHA\",\"PCAHA\"]\n",
    "\n",
    "    for i, value in enumerate(df[column]):\n",
    "        if pd.isna(value): \n",
    "            if backup_column is None: # Avg or Max column\n",
    "                if column.startswith(\"Avg\"): \n",
    "                    imputed_value = df.loc[i, original_columns].mean()\n",
    "                elif column.startswith(\"Max\"): \n",
    "                    imputed_value = df.loc[i, original_columns].max()\n",
    "            else: \n",
    "                if pd.isna(df.loc[i, backup_column]): \n",
    "                    imputed_value = df.loc[i, original_columns].mean()\n",
    "                else: \n",
    "                    imputed_value = df.loc[i, backup_column]\n",
    "        \n",
    "            print(i, column,imputed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
